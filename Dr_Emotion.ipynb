{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[1], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone discriminator model\n",
    "def get_discriminator():\n",
    "    #in_condition refers to estimated embedding from G\n",
    "    in_condition = tf.keras.Input(shape=(768,))\n",
    "    fe = tf.keras.layers.Dense(128)(in_condition)\n",
    "    fe = tf.keras.layers.ELU()(fe)\n",
    "    fe = tf.keras.layers.Dropout(0.2)(fe)\n",
    "    fe =  tf.keras.layers.Dense(64)(fe)\n",
    "    fe = tf.keras.layers.ELU()(fe)\n",
    "    fe = tf.keras.layers.Dropout(0.2)(fe)\n",
    "    fe =  tf.keras.layers.Dense(16)(fe)\n",
    "    fe = tf.keras.layers.ELU()(fe)\n",
    "    fe = tf.keras.layers.Dropout(0.2)(fe)\n",
    "    # 8 emotion, 1 neutral, 1 fake/real\n",
    "    out_layer1 = tf.keras.layers.Dense(1, activation='sigmoid', name='fr')(fe)\n",
    "    out_layer2 = tf.keras.layers.Dense(1, activation='sigmoid',name='condition')(fe)\n",
    "    out_layer3 = tf.keras.layers.Dense(1, activation='sigmoid',name='class_label')(fe)\n",
    "    model = tf.keras.Model(in_condition, [out_layer2, out_layer1, out_layer3])\n",
    "    opt = tf.keras.optimizers.Adam(lr=0.0002)\n",
    "    \n",
    "    model.compile(loss={'condition': 'mse', 'fr': 'binary_crossentropy', 'class_label': 'binary_crossentropy'}, loss_weights={'condition': 1., 'fr': 1., 'class_label': 1.}, optimizer=opt)\n",
    "    return model\n",
    "\n",
    "def get_generator():\n",
    "    # interface for neutral indicator \n",
    "    in_label1_input = tf.keras.Input(shape=(1,))\n",
    "    # interface for noise\n",
    "    in_label2_input = tf.keras.Input(shape=(32,))\n",
    "    in_embedding = tf.keras.Input(shape=(768,))\n",
    "    gen = tf.keras.layers.Dense(512)(in_embedding)\n",
    "    gen = tf.keras.layers.LeakyReLU()(gen)\n",
    "    gen = tf.keras.layers.Dense(198)(gen)\n",
    "    gen = tf.keras.layers.LeakyReLU(name = 'd_embed')(gen)\n",
    "    merge = tf.keras.layers.Concatenate()([in_label1_input,in_label2_input, gen])\n",
    "    gen = tf.keras.layers.Dense(198)(merge)\n",
    "    gen = tf.keras.layers.LeakyReLU()(gen)\n",
    "    gen = tf.keras.layers.Dense(512)(gen)\n",
    "    gen = tf.keras.layers.LeakyReLU()(gen)\n",
    "    out_layer = tf.keras.layers.Dense(768, activation='linear')(gen)\n",
    "    model = tf.keras.Model([in_label1_input,in_label2_input, in_embedding], out_layer)\n",
    "    return model\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "    # get noise and label inputs from generator model\n",
    "    in_label1,in_label2, in_embedding = g_model.input\n",
    "    d_model.trainable = False\n",
    "    # in_label1: emotion scores\n",
    "    # in_label2 gaussian noise\n",
    "    # in_embedding: original embedding \n",
    "    \n",
    "    # get image output from the generator model\n",
    "    gen_output = g_model.output\n",
    "    # connect image output and label input from generator as inputs to discriminator\n",
    "    gan_output1, gan_output2, gan_output3  = d_model(gen_output)\n",
    "    \n",
    "    # define gan model as taking noise and label and outputting a classification\n",
    "    model = tf.keras.Model([in_label1,in_label2, in_embedding], [gan_output1, gan_output2, gan_output3])\n",
    "    # compile model\n",
    "    opt = tf.keras.optimizers.Adam(lr=0.0002)\n",
    "#     opt = tf.keras.optimizers.Adadelta()\n",
    "    model.compile(loss={d_model.name: 'mse', d_model.name+'_1': 'binary_crossentropy', d_model.name+'_2': 'binary_crossentropy'}, loss_weights={d_model.name: 1., d_model.name+'_1': 1.,  d_model.name+'_2': 1.}, optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = get_generator()\n",
    "d = get_discriminator()\n",
    "gan = define_gan(g, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "def generate_real_samples_for_d(embeddings, scores, n_samples, current_batch):\n",
    "    # embedding \n",
    "    ix = randint(0, embeddings.shape[0], n_samples)\n",
    "#     embedding = embeddings[current_batch*n_samples:(current_batch+1)*n_samples]\n",
    "    embedding = embeddings[ix]\n",
    "    score = []\n",
    "    label = []\n",
    "    for index in range(n_samples):\n",
    "        score.append(np.array([0]))\n",
    "        label.append(np.array([1]))\n",
    "    # neutral score\n",
    "    for index in range(n_samples):\n",
    "        score[index][0] = scores[ix[index]]\n",
    "#         score[index][0] = random.uniform(0, 1)\n",
    "    return embedding, score, label, train_label1[ix]\n",
    " \n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(n_samples):\n",
    "    z_input = []\n",
    "    noise = []\n",
    "    for _ in range(n_samples):\n",
    "        noise.append(list(generate_known_gaussian(32)[0]))\n",
    "    return noise, z_input\n",
    " \n",
    "def generate_fake_samples_for_d(embeddings, scores, n_samples, current_batch, generator):\n",
    "    # embedding \n",
    "    ix = randint(0, embeddings.shape[0], n_samples)\n",
    "    embedding = embeddings[ix]\n",
    "    score = []\n",
    "    label = []\n",
    "    for _ in range(n_samples):\n",
    "        score.append(np.array([0]))\n",
    "        label.append(np.array([0]))\n",
    "    # neutral score\n",
    "    for index in range(n_samples):\n",
    "        score[index][0] = scores[ix[index]]\n",
    "    noise, z_input = generate_latent_points(n_samples)\n",
    "    fake_embedding = generator.predict([score, noise ,embedding])\n",
    "    return fake_embedding, score, label, train_label1[ix]\n",
    "\n",
    "def define_encoder(g_model):\n",
    "    in_label1,in_label2, in_embedding = g_model.input\n",
    "    output = g_model.get_layer(\"d_embed\").output\n",
    "    model = tf.keras.Model([in_label1,in_label2, in_embedding], output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, embeddings, scores, n_epochs, test_embedding, n_batch=64):\n",
    "    bat_per_epo = int(embeddings.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    for i in range(n_epochs):\n",
    "        for j in range(bat_per_epo):\n",
    "            embedding_real, score_real, label_real, cls_label = generate_real_samples_for_d(embeddings, scores, half_batch, j)\n",
    "            d_loss1 = d_model.train_on_batch(embedding_real, [score_real, label_real, cls_label])\n",
    "            fake_embedding, score_fake, label_fake, cls_label = generate_fake_samples_for_d(embeddings, scores, half_batch, j, g_model)\n",
    "            d_loss2 = d_model.train_on_batch(fake_embedding, [score_fake, label_fake, cls_label])\n",
    "            ix = randint(0, embeddings.shape[0], n_batch)\n",
    "            embedding = embeddings[ix]\n",
    "            noise, z_input = generate_latent_points(n_batch)\n",
    "            score = []\n",
    "            label = []\n",
    "            for _ in range(n_batch):\n",
    "                score.append(np.array([0]))\n",
    "                label.append(np.array([1]))\n",
    "            # neutral score\n",
    "            for index in range(n_batch):\n",
    "                score[index][0] = scores[ix[index]]\n",
    "            g_loss = gan_model.train_on_batch([score, noise, embedding], [score, label, train_label1[ix]])\n",
    "            total_gan_loss.append([g_loss[0], (g_loss[1]+g_loss[2])/2, (d_loss1[0]+d_loss2[0])/2, (((d_loss1[1]+d_loss1[2])/2) + ((d_loss2[1]+d_loss2[2])/2))/2 ])\n",
    "            print('>%d, %d/%d, g_loss=%.3f, g_condition=%.3f, d_loss=%.3f, d_condition=%.3f' %(i+1, j+1, bat_per_epo, g_loss[0], (g_loss[1]+g_loss[2])/2, (d_loss1[0]+d_loss2[0])/2, (((d_loss1[1]+d_loss1[2])/2) + ((d_loss2[1]+d_loss2[2])/2))/2),end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_d1_loss = []\n",
    "total_d2_loss = []\n",
    "total_gan_loss = []\n",
    "total_d_loss = []\n",
    "train(g,d,gan,embedding,neutral_score,100, embedding2, 16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
